\section{Our Approach}\label{sec:approach}
\begin{wrapfigure}{R}{0.5\textwidth}
\begin{algorithm}[H]
\scriptsize
\SetKwProg{Fn}{}{ \{}{\}}
\fontsize{8.2pt}{5pt}\selectfont{
\SetAlgoLined
int  changed = 0;  // Global variable \label{line:globdecl1}}\\
\Fn(){\textbf{relaxgraph}(Point  p, Graph  graph)} {
                        \textbf{foreach} (t In p.outnbrs)\label{line:foreach21}\\
        \hspace{0.05in} MIN(t.dist, p.dist + graph.getweight(p, t), changed);    \label{line:modidist1}\\%uses \texttt{atomicMin()}
}
\Fn(){\textbf{main}(int argc, char *argv[])} {
       ......\\
        \While {(1)}{     %\todo{can we avoid infinite loop}      \\
         changed = 0;           \\\label{line:initchanged1}
                \textbf{foreach} (t In graph.points)  relaxgraph(t,graph);\label{line:relaxfun1}\\
                if (changed == 0) break;        //reached fix point\label{line:checkexit1}\\

        }\label{line:ssspendlopp1}
        %for (int i = 0; i \textless graph.npoints; ++i)        \label{line:printdist}\\
        %       \hspace{0.3in}printf("i=\%d dist=\%d$\backslash$n", i, graph.points[i].dist);

        }
\caption{SSSP: iterating over Points in Falcon}
\label{approach:algo1}
\end{algorithm}
\end{wrapfigure}

Algorithm~\ref{approach:algo1} shows the code fragment of the SSSP computation in Algorithm~\ref{background:algo1}.
Only parts  which are relevant for our compiler transformation are shown here.
The algorithm is vertex-based SSSP computation using {\it points} and {\it outnbrs} iterators.
If the target device is specified as \CPU, then parallel C++ code with {\tt OpenMP} gets generated. A  programmer may request  vertex-based code using the relevant command line argument  during compilation of the above DSL code. The compilation happens similar to the original Falcon compiler and the output would be the same as that generated by Falcon.

A programmer may require parallel C++ code which is similar to the parallel C++ SSSP generated by Falcon using {\it edges} iterator (see Algorithm~\ref{background:algo2}). In Falcon, the programmer has to code it separately. 
This means, one needs to write a separate code for vertex-based processing, another code for edge-based processing, yet another for asynchronous variants of these, and so on.
It would be ideal if the programmer simply specifies \textit{what} rather than \textit{how}, and the DSL compiler takes care of appropriate code generation.
We support it in this work.
In our proposal, the programmer simply needs to specify different compile-time arguments. 
This triggers generation of parallel C++ code  matching the output of the Falcon compiler for Algorithm~\ref{background:algo2}. Thus, coding efforts considerably reduce.

Unnikrishnan et al.~\cite{falcon} discuss how Falcon converts a DSL code matching the iterators present in the DSL code. Here, we explain our compiler transformation. That is  {\it points}+{\it outnbrs} based DSL code generates parallel C++ code which matches the output of {\it edges} iterator  based Falcon DSL code.
The \texttt{foreach} calls at Lines~\ref{line:relaxfun1} and ~\ref{line:foreach21} make all the edges of Algorithm~\ref{approach:algo1} to get processed in the code enclosed inside the {\it outnbrs} iterators.
 Our compiler transformation removes the {\tt foreach} in relaxgraph function (Line~\ref{line:foreach21} and converts the \texttt{foreach} in Line~\ref{line:relaxfun1} to edges iterator. The first argument to {\it relaxgraph()} is modified to an  {\tt Edge} object from a {\tt Point} object.
 In Algorithm~\ref{approach:algo1} the iterator instances with name  {\it p} and
 {\it t}
 act as source and destination vertices of the edge $p\rightarrow t$ of the graph. 
Thus, in the relaxgraph function, we instantiate {\it p} and {\it t} using source and destination vertices of the edge. Note that our transformation has modified the first argument to relaxgraph as of type edge now. 
This is followed by the Falcon compiler generating the parallel C++/CUDA code.
This transformation requires modification of Falcon Abstract Syntax tree (AST).
We now highlight the challenges faced in generating code from the same DSL code.

\begin{wrapfigure}{R}{0.63\textwidth}
\begin{algorithm}[H]
\scriptsize
\KwIn{Falcon vertex/edge based DSL code}
\KwOut{C++/CUDA edge or vertex based code based on  target}
 \textbf{ begin  Step1:-} Mark  outermost {\tt foreach} statement (Done in  Falcon Parser).\\
 \If {  statement.type=={\tt foreach} \&\& level==0} {
   t.outer={\tt true};\\
}
 \If {  statement.type=={\tt foreach} \&\& level==1} {
   t.outer={\tt false};\\
}
 \textbf{end Step1}\\
 \textbf{begin Step2:-}  Convert vertex code to edge code \\
  \ForAll { {\tt foreach} statement $f1$ in $program$ } {
 \If { $f1$.outer== {\tt true} \&\& $f1$.iterator==$points$}{
  \ForAll { {\tt foreach} statement $f2$ in $program$ } {
  \If{$f2.def.fun==f1.call.fun$ \&\& $f2.itr==outbrs$ $||$ $f2.itr==innbrs$}{
  //modify iterator of $f1$  to points\\
  // modify  $1^{st}$ argument to {\tt Edge} in    $f2.def.fun$\\
  //create  $f2.itr$ and $f1.itr$ in  $f2.def.fun$ using {\tt Edge}.\\
  // remove {\tt foreach} in $f2.def.fun$\\
 // generate code (Done by Falcon)\\
  }
}
}
}
\textbf{end Step2}\\
  \textbf{begin Step3:-} Convert edge code to vertex code\\
  \ForAll { {\tt foreach} statement $f1$ in $program$ } {
 \If { $f1$.outer=={\tt true} \&\& $f1$.iterator==$edges$}{
  \textbf{function} fun =$f1.call.fun$ \\
  modify iterator of fun  to points\\
  // modify $1^{st}$ argument of fun to {\tt Point}\\
  // insert $outnbrs$ or $innbrs$  iterator in fun using {\tt Point}\\
 // generate code (Done by Falcon)\\
  }
}
\textbf{end Step3}
  \caption{Code Transformation}
 \label{codege:optcomm}
 \end{algorithm}
 \end{wrapfigure}

\subsection{Vertex-based versus Edge-based}\label{sec:vertexedge}
Conversion of vertex-based to edge-based and vice-versa are done completely at the abstract-syntax tree (AST) level  by traversing the AST and modifying its eligible parts. An important conversion non-triviality stems from the edge-based processing being a single loop, while the corresponding vertex-based processing is a nested loop (outer loop over vertices, and inner loop over all the neighbors of each vertex).
Pseudo-code for the transformation is presented in Algorithm~\ref{codege:optcomm}. Step-1 in the algorithm marks the outer \textit{foreach} statements. Step-2 transforms vertex-based code to edge-based code. Step-3 transforms edge-based code to vertex-based code.

In vertex-based to edge-based conversion,  the subtree is eligible for conversion if the following two conditions are met. 
First, the subtree is rooted at a function node whose only child is a node for a \texttt{foreach} which iterates through a point's neighbors. 
Second, the function is the only statement in the body of a \texttt{foreach} which iterates through the graph-points. 
Once the eligible parts are found, we switch the points iterator of the \texttt{foreach} statement from which the function is called to edge iterator, and then remove the \texttt{foreach} statement in the function. 
The conversion also requires change in the function's signature as its argument was earlier a point, while now it is an edge. 
It also necessitates defining two new variables at the beginning of the function corresponding to the source and the destination of the edge. 
The name of one of the two variables is the name of the point which was the former parameter of the function. 
The other variable's name is derived from the iterator of the \texttt{foreach} statement removed from the function earlier. 
The order in which these names are mapped to the variables depends on the iterator used in the removed \texttt{foreach}. 
If the iterator is over out-neighbors, the name of the iterator is mapped to the destination vertex of the edge.
Otherwise, we map it to the source vertex. 
Such an implementation allows the rest of the processing in the iteration to be arbitrary, and reduces the number of alterations the compiler needs to perform to the code.
%By implementing this way we do not have to alter any other statements in the function as these statements can still access the source and destination vertex with same name as it was before.

In edge-based to vertex-based conversion, the compiler needs to do the opposite (see Algorithm~\ref{codege:optcomm}). 
Here, it finds the \texttt{foreach} statement iterating through the edges of a graph which contains a function call statement as the only statement in the loop-body. 
For such a \texttt{foreach}, the iterator is changed to iterate over points and a new \texttt{foreach} statement iterating through the neighbors of the point is introduced in the called function (which encloses the body of the called function). 
This conversion necessitates the parameter of the called function to be changed from edge type to point type. 
It also requires defining a new variable which represents the edge between the point passed as a parameter and the iterator which represents the point's neighbor. 
Such an implementation also allows the rest of the processing in the iteration to be arbitrary.

An important artifact of this processing conversion is that it affects the way graph is stored in memory.
In vertex-based code, Falcon (and other frameworks) store the graph in compressed sparse-row (CSR) format. 
Compared to edge-list representation, CSR format reduces the storage requirement and has the benefit of quick access to the out-neighbors of a vertex. 
In edge-based codes, on the other hand, graph is stored in edge-list format (\textsf{source destination weight}) which enables quick retrieval of the source and the destination points of an edge.
However, one graph format also forbids the other format's advantages.
For instance, if the graph is stored in CSR format, retrieving an arbitrary edge (say, edge $p \rightarrow q$) is time-consuming (requires search over the out-neighbors of $p$).
On the other hand, if the graph is stored as edge-list, retrieving an edge, given the source and the destination vertices is even more time-consuming, as the edges are not indexed on vertices. 
In many cases (and all in our testbed), accessing edges is not arbitrary. 
Edges are accessed in a particular order, for instance, iterating through out-neighbors of a point. 
So edge between a point and its out-neighbor can be retrieved quickly if the graph is stored in CSR format. 
Unfortunately, while changing vertex-based code to edge-based code, accessing edges in this manner becomes time-consuming. 
To overcome this, the code where the edge is retrieved is replaced by the edge passed as an argument to the transformed function. 
This resolves the issue of enumerating over neighbors.
%This only resolves the problem created from the ordered way of edge access. If there are arbitrary access of edges, vertex-based as well as edge-based will face performance issue.

\subsection{Synchronous versus Asynchronous}\label{sec:syncasync}
By default, Falcon generates synchronous code, that is, it inserts a barrier at the end of parallel construct. 
While this works well in several codes and eases arguing about the correctness (due to data-races restricted to within-iteration processing across threads), synchronous processing may be overly prohibitive in certain contexts.
Especially, in cases where processing across iterations is independent and the hardware does not necessarily demand single-instruction multiple data (SIMD) execution, asynchronous processing may improve performance.
Arguing about the correctness-guarantees gets so involved in asynchronous execution, that some DSLs strictly enforce synchronous code generation only.
Our proposal is to allow the programmer to generate synchronous or asynchronous code without having to change the algorithm specification code in the DSL.
Achieving this necessitates identifying independent processing in the code. 
Towards this, we maintain read and write sets of global variables and the graph attributes used in each \textit{target function} separately. A \textit{target function} is a function which is being called in the body of a \texttt{foreach} statement and the function call is the only statement inside the body of the \texttt{foreach}.  On CPU, it is the parallel loop body, while on GPU, this function becomes the kernel.

This is a two-step process. In Step 1, we mark nodes in the control-flow graph (CFG); and in Step 2, we generate the appropriate code.
In Step 1, we construct the CFG of the \textit{target function} call.
Using the read and the write sets corresponding to each of the \textit{target function}s, we mark each node of the CFG as \textit{barrier-free} or not. A barrier-free node signifies that the \textit{target function} corresponding to the barrier-free node can be executed concurrently with the children of this node.
A node is barrier-free if it satisfies the following two conditions:
\begin{enumerate}
\item There is no dependency (Read-Write, Write-Read and Write-Write) between the node and each of its children. 
\item There is no dependency between the node and the codes between the node and its children.
\end{enumerate}
%If a node satisfies both the conditions, we mark the node as barrier free, otherwise we mark it as not barrier free.
If a node is barrier-free, we pass the read and the write sets of the node to its children. 
We do this so that the grand-child should not have any dependency with the grand-parent node to declare its parent barrier free (and so on).
% This is illustrated in Figure-1.
We follow this process to mark all the CFG nodes in breadth-first search order.

In Step 2, based on the target code the user wants, different procedures are followed to make the code asynchronous.
If the target is GPU, all the nodes marked as barrier-free do not contain a barrier \textit{cudaDeviceSynchronize()} after the kernel launch. 
Also, each of the barrier-free kernels is launched in different streams of the same GPU.
On the other hand, if the target is CPU, the \textit{target function} call corresponding to the barrier-free node is put in a section of an OpenMP parallel region, and its children and the code between the node and its children in another section.
The compiler then recursively checks if the child nodes are barrier-free or not. 
If they are, then a new OpenMP \texttt{parallel sections} construct is introduced inside the section where the child was put in earlier, because of its barrier-free parent node.
This recursive introduction of parallel sections continues until a non-barrier-free node is found, or until the processing reaches the end of the function where these \textit{target functions} are called. 
The introduction of OpenMP constructs is done by adding new nodes in the AST.
For a parallel region, two nodes are added: one each for the start and the end of the construct. In a similar manner, for each section, a node for the start and another node for the end is added. 
Adding these nodes is easy if both the node and its children in the CFG lie in the same scope. 
We can then simply add a node prior to and another node right after the barrier-free node. 
The processing gets involved when a node and its children are in different scopes.
In such cases, we need to find the predecessors of these nodes which lie in the same scope.
% Both these cases are illustrated in the Figure-2.

\textbf{Discussion:} The way we group two parallel constructs may not lead to optimal grouping. 
Finding the optimal grouping is not feasible as it depends on the time required to execute the parallel constructs. 
For instance, $A$, $B$, and $C$ are parallel constructs where $A$ and $B$ are independent, $B$ and $C$ are independent, and $A$ and $C$ are dependent. 
If time required to execute $A$ is less compared to $B$ and $C$, then grouping $A$ and $B$ does not lead to optimal performance.
On the other hand, if the time required to execute $C$ is less compared to $A$ and $B$, then grouping $A$ and $B$ does lead to optimal performance.


\subsection{CPU, GPU and Multi-GPU Codes}\label{sec:cpugpu}
Falcon requires users to write different DSL code for different backends.
It uses $<$GPU$>$ tag to specify a GPU variable.
Falcon compiler generates GPU code if there exists a GPU variable in the program and converts function to GPU kernel if one of the parameters is a GPU variable.
We modified the Falcon grammar so that compiler does not need a GPU tag.
It recognizes a device-independent version of the DSL code.
Based on the command-line argument, our compiler generates code for an appropriate target device.

The compiler generates the GPU code in the following manner.
First, it marks all the \textit{target functions} as kernels.
Second, it marks the global variables used in the \textit{target functions} as GPU variables.
Third, it makes a GPU copy of each of the variables of type graph, set and collection.
Fourth, it replaces the CPU copy of a graph, set or collection with its corresponding GPU copy.

To generate multi-GPU code, the user has to use \texttt{parallel sections} construct of Falcon.
Compiler assumes that each of the sections is independent of each other.
We identify the number of sections in a parallel sections construct and map each of the sections to a different GPU.
For each graph, set and collection used in a particular section, a GPU copy is created in the mapped GPU.
It may happen that user has used a single graph and used that graph in multiple sections.
In such cases, the graph needs to be copied to each GPU.
%Each of those GPU copies is allocated to different GPU as each of the sections is mapped to different GPU.
For each of those GPU copies, we keep track of the attributes of the graph or its points/edges used in the target functions where the graph is passed as an argument.
This helps us to replace the graph whose attribute is accessed in CPU by the appropriate GPU copies where the accessing attribute is present.
%By replacing the CPU graph with its appropriate GPU copy, we are able to keep the logic of the program correct.
Now if an attribute of a GPU graph is accessed in the CPU, the Falcon compiler generates a call to \texttt{cudaMemcpy} to copy the attribute from GPU to CPU or from CPU to GPU based on whether user has read or written to the attribute.
One advantage of assuming independent sections is that the attributes accessed in CPU can be changed on maximum one GPU.
This eases our analysis and code-generation.
%This is because each of the sections is assumed to be independent. So no two section of a parallel sections will modify the same attribute.


\subsection{Data-Transfer Aggregation Optimization}\label{sec:optimizations}
In Falcon, custom property of points/edges in a graph is stored as an array where $n^{th}$ position stores the value corresponding to point/edge $n$.
If a property of a point/edge of a GPU graph is read in CPU, a data transfer from GPU to CPU is necessary.
For $n$ such reads, $n$ instances of data transfer are required.
This is not always desirable and can quickly become a performance bottleneck due to slow PCI-e across the devices. 
To overcome this, we combine multiple such data transfers into a single data transfer, where feasible.

Such an optimization is particularly beneficial for \textit{for} and \textit{while} loops.
%The pseudo-code for our data-transfer aggregation optimization is presented in Algorithm~\ref{codege:optcomm}.
If the instructions inside the body of a \textit{for/while} loop only read a particular property (and may write to other properties), we copy the array related to that property to CPU before the beginning of the loop using a single data transfer instruction. 
The instructions in the loop then use the CPU copy.
This often improves the performance as a loop is expected to be executed several times.


